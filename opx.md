You are an expert systems programmer.

TASK
Create two small command line programs:
- opx.sh (bash)
- opx.py (python)

Both programs MUST be functionally identical.
Differences are allowed only where required by the language.

LANGUAGE CONSTRAINTS
- Bash: bash-compatible, POSIX-style, no external tools except curl
- Python: Python 3.12, standard library only
- No third-party libraries
- No SDKs

GENERAL BEHAVIOR
The programs act as a local OpenAI-compatible LLM client.

DEFAULTS
- host: localhost
- port: 11434
- model: llama3.2:3b

CLI CONTRACT
Mandatory argument:
- A prompt string

Options:
-m <model>      model name
-h <host>       hostname
-p <port>       port number
-o <file>       write output to file instead of stdout
-c              output ONLY code blocks
-e <file>       read file content instead of stdin
--help          show help and exit

INPUT HANDLING
- If stdin is not empty and -e is not used, read stdin and append it to the prompt
- If -e is used, read the given file and append its content to the prompt
- Content from stdin or -e MUST be appended enclosed in triple backticks
  and prepended with two line feeds.

SYSTEM PROMPT
Always prepend the following system prompt to the user prompt:
"Be a mighty Linux system operator. Use short answers. If code or commands are requested, output only code."

API CONTRACT
- Use HTTP POST
- Endpoint: http://<host>:<port>/v1/chat/completions
- Content-Type: application/json
- No Authorization header

The request MUST use:
- model
- messages with system and user roles
- stream=true for outputs that are given to stdout, stream=false for outputs to a file.

OUTPUT HANDLING
- Stream the response to stdout or to the output file
- Do not buffer the full response before writing

CODE FILTERING (-c)
- Extract ONLY content inside triple-backtick code blocks
- Ignore language identifiers after the backticks
- If multiple code blocks exist, concatenate them with newlines
- If no code block exists, output nothing

EDIT MODE (-e with -c)
- Overwrite the input file with the filtered output


TOOLING SUPPORT

Tool name: bash

LLM TOOL REQUEST FORMAT:
If the LLM wants to run a shell command, it MUST output exactly one JSON object:

{
  "tool": "bash",
  "command": "<single shell command>"
}

TOOL SAFETY RULES:
- One command only
- No pipes (|)
- No command chaining (;, &&, ||)
- No background execution
- No redirection

USER APPROVAL FLOW:
1. Detect a tool request from the LLM
2. Display the command to the user
3. Ask for explicit confirmation (yes/no)
4. If rejected, abort tool execution
5. If approved, execute the command locally

TOOL RESULT FORMAT (sent back to LLM):

{
  "tool": "bash",
  "exit_code": <int>,
  "stdout": "<captured stdout>",
  "stderr": "<captured stderr>"
}

The tool result MUST be appended to the conversation
as a tool response before continuing the LLM stream.

OUTPUT HANDLING
- Stream all non-tool LLM output to stdout or -o file
- Tool execution output is NOT printed unless explicitly generated by the LLM

CODE FILTERING (-c)
- Extract only triple-backtick code blocks
- Ignore language identifiers
- Concatenate multiple blocks
- If none exist, output nothing

EDIT MODE (-e with -c)
- Overwrite the input file with the filtered output

ERROR HANDLING
- Invalid options: print usage and exit non-zero
- Tool execution errors: still return stdout/stderr to the LLM
- Network errors: short message to stderr

PROHIBITIONS
- Never auto-execute commands
- Never execute commands without user approval
- Never simulate command output
- Never retry commands implicitly
